# -*- coding: utf-8 -*-
"""PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10iJrRXJO4h8LcVZdLSUNPlF0ggxM7U73

#Import
"""

import torch

torch.__version__

"""#Types of Tensors"""

list1 = [1,2,3]
t1 = torch.tensor(list1)
t1

type(t1)

import numpy as np
np_array = np.array([1,2,3])
t2 = torch.tensor(np_array)
t2

t3 = torch.ones(2)
t3

?torch.ones

t4 = torch.zeros(size = (2,2))
t4

t5 = torch.rand((2,2))
t5
#gives no. btw 0 and 1

torch.manual_seed(0)
t5 = torch.rand((2,2))
t5
#fixing the values just like random state

t6 = torch.randint(0,5,size  = (2,2))
t6
#int from 0 to 4 randomly

t7 = torch.arange(start=0, end=10, step=2)
t7

t7 = torch.arange(0,10,2)
t7

t8 = torch.arange(0,9,1).reshape(3,3)
t8

t9 = torch.zeros_like(t8)
t9
#shape of t8 but all 0

t10 = torch.ones_like(t8)
t10
#shape of t8 but all 1

"""#Properties of tensors"""

# t.shape
# t.ndim
# t.dtype
# t.device

t1 = torch.tensor(4)
t2 = torch.tensor([1,2,3,4])
t3 = torch.tensor([[1,2],[3,4]])
t4 = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])

print(t1.shape)
print(t2.shape)
print(t3.shape)
print(t4.shape)

print(t1.ndim)
print(t2.ndim)
print(t3.ndim)
print(t4.ndim)

print(t1.dtype)
print(t2.dtype)
print(t3.dtype)
print(t4.dtype)

print(t1.device)
print(t2.device)
print(t3.device)
print(t4.device)

#Types of datatypes
# torch.float32
# torch.float64
# torch.intt32
# torch.int64

"""We cannot keep mixed datatype in a tensor. datatype of tensor depends on datatype of its elements. If we have mixed elements in a tensor then datatype of tensor changes

Bool & Int are converted to int


Int & float are converted to Float


Int & Str will throw error as it cannot be type casted


if int 32 & int64 is mixed then overall converted to int64 as overcast

"""

t1 = torch.tensor([1,2,3])
t1.dtype

t1 = torch.tensor([1.,2,3])
t1.dtype

#We can manually typecast
t1 = torch.tensor([1,2,3] , dtype = torch.float64)
t1.dtype

print(t1.device)
print(t2.device)
print(t3.device)
print(t4.device)

"""#GPU Check"""

#CUDA
torch.cuda.is_available()

torch.cuda.get_device_name()

torch.cuda.device_count()

t1.to('cuda')

"""#Accessing elements in a Tensor

How to access elements in a tensor
"""

t = torch.tensor([[1,2,3],[4,5,6]])
t

t[0]

t[1,2] #similar to t[1][2]

t[0][0]

t = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])

t[:2] # 0 and 1 element

t[:,-1] #selected all rows and last elements of each row

t[1,1:] # row1 and element 1 & 2

t = torch.tensor([1, 2, 2, 5, 6])
mask = t > 3
mask

t[mask] # mask act as a filter for the elemennts in the tensor

"""#Operations on Tensors"""

t = torch.tensor([1, 2, 3])
print(t + 10) #operation gets applied to each elememt similar to numpy
print(t - 10)
print(t * 10)
print(t / 10)

t = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(t)
print(torch.sum(t))
print(t.sum())
#sum function flattens the tensor & results sum of all the elements, we can specify dimensions as well

print(torch.sum(t, dim=0)) # result to be a row
print(torch.sum(t, dim=1)) # result to be a column

"""Getting Max & Min"""

t = torch.tensor([[1, 2, 3], [4, 1, 6]])
print(t)
print(torch.max(t))
print(t.max())

print(torch.max(t, dim=0)) # result to be a row
print(torch.max(t, dim=1)) # result to be a column

"""Multiplication"""

a = torch.tensor([1,2,3])
b = torch.tensor([4,5,6])
print(torch.mul(a,b)) # element wise multiplication
print(a*b)

"""Matrix Multiplication"""

A = torch.tensor([[1, 2], [3, 4]])
B = torch.tensor([[2, 0], [1, 2]])

print(torch.matmul(A,B)) # matrix multiplication
print(A@B)

# torch.save()
# torch.load()

torch.save(t, 'tensor_t.pt')

t1 = torch.load('tensor_t.pt')
t1

#To access individual element of a scaler tensor
c = torch.tensor(5)
print(c)
c.item()

# To check if 2 tensors are close to each other
# torch.allclose()

t = torch.empty(3)
print(t)
print(t.shape)

t = torch.empty(3).random_(2)
t

#Equally spaced values let say for hyper parameter tuning
torch.linspace(-5,5,10)

#Copy of a tensor
t1 = torch.tensor([1, 2, 3])
t2 = t1 # this is soft copy (just renaming)
t3 = t1.clone() # any change in t3 will not be reflected in t1 (hard copy)

"""#Autograd (automatically done gradient calculation)"""

#.backward()

#Autograd doen not works on int, that's why it is required to give parameter of x as float
x = torch.tensor(2.0, requires_grad = True) # until this parameter is not True, gradient will not be calculated
y = 5*x**3 + 2*x**2 + 3

y.backward() #we need to call this in order to calculate gradient
x.grad # gradient is stored in x.grad

x = torch.tensor(2.0, requires_grad = True)
y = torch.tensor(3.0, requires_grad = True)
z = 5*x**3 + 2*x**2 + 3

z.backward()
print(x.grad)
print(y.grad) # z is not a function y hence None for this

#here one of the two variable's parameter is True hence that was calculated and other gives None. Here if both were False then it will throw an error
x = torch.tensor(2.0, requires_grad = True)
y = torch.tensor(3.0, requires_grad = False)
z = x**2 + y**2

z.backward()
print(x.grad)
print(y.grad) # Parameter is False

x = torch.tensor(2.0, requires_grad = False)
w = torch.tensor(2.0, requires_grad = True)
b = torch.tensor(2.0, requires_grad = True)

y = w * x + b
y.backward()

print(x.grad)
print(w.grad)
print(b.grad)

#We can check as well
b.requires_grad

x = torch.tensor(10.0, requires_grad = True)
y = 2*x**2
y.backward()
x.grad

#After we apply compuation through a tensor the result will be a tensor
print(y)

x = torch.ones(2, 2, requires_grad = True)
y = x + 2
z = y * y * 3
out = z.mean()
out.backward() # x is defined tensor with req_grad as True, y is defined through x, z by y and out by z

print(x.grad)
print(y.grad)
print(z.grad)

print(x.is_leaf)
print(y.is_leaf)
print(z.is_leaf)
print(out.is_leaf)

"""X is a leaf tensor(node). The tensors that we create ourself are always leaf nodes irrespective of required_grad.


Tensors which are intermediate are leaf or not depends on if requires_grad is False of the leaf node(from where they are defined).

If created nodes requires_grad is False then intermediate are leaf and vice versa


"""

a = torch.rand(2,2, requires_grad = False)
b = a + 2

print(a.is_leaf)
print(b.is_leaf)

a = torch.rand(2,2, requires_grad = True)
b = a + 2

print(a.is_leaf)
print(b.is_leaf)

#retain_grad()

x = torch.ones(2, 2, requires_grad = True)
y = x + 2
z = y * y * 3
out = z.mean()

print(x.is_leaf)
print(y.is_leaf)
print(z.is_leaf)

out.backward()

print(x.grad)
print(y.grad)
print(z.grad)

#Since y & z was not leaf so no grad for them but if we retain it and then run again it will be calculated
y.retain_grad()
z.retain_grad()

out.backward()

print(x.grad)
print(y.grad)
print(z.grad)

"""# Different ways to call activation functions

*   relu (torch.relu())
*   sigmoid (torch.sigmoid())
*   tanh (torch.tanh())

---------------------------------------------


1.   torch.nn.Softmax
2.  torch.nn.LeakyReLU
3. torch.nn.ELU
4. torch.nn.GELU
5. torch.nn.ReLU
6. torch.nn.Sigmoid
7. torch.nn.Tanh


------------------------------------------------

- torch.nn.functional.relu()
- torch.nn.functional.sigmoid()
- torch.nn.functional.tanh()
- torch.nn.functional.softmax()
- torch.nn.functional.leaky_relu()
- torch.nn.functional.elu()
- torch.nn.functional.gelu()
"""

import torch.nn as nn
import torch.nn.functional as F

x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
print(torch.relu(x))

relu = nn.ReLU()
print(relu(x))

print(F.relu(x))

"""# Different ways to call loss functions

* torch.nn.MSELoss()
* torch.nn.CrossEntropyLoss()
* torch.nn.BCELoss()
"""

y_pred = torch.tensor([1., 2., 3., 4.])
y_true = torch.tensor([2, 2, 2, 2], dtype = torch.float32)

mse = nn.MSELoss()
loss = mse(y_pred,y_true)
loss.item()

"""#Optimization Functions"""

import torch.optim as optim

#optimizer  = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9, nesterov=True)

#optimizer = optim.Adam(model.parameters(), lr = 0.001)

#optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha =0.99)

# optimizer = optim.AdaGrad(model.parameters(), lr = 0.01)

"""#ANN

##Initialization
"""

nn.Linear #helps to create layers in ANN
#when we run this, some random values are used to initialise weights and biases

"""in_features (no. of features present in each sample)

out_features (no. of values in the outputs)

bias (its a boolean value which denotes if we want to add bias or not)
"""

import torch
import torch.nn as nn
layer1 = nn.Linear(in_features=3,out_features=2,bias=True) #bias is true by default

input_tensor = torch.tensor([1.0, 2.0, 3.0])
output_tensor = layer1(input_tensor)
output_tensor

print(layer1.weight) #these are the weights that was initialsed randomly
print(layer1.bias) #same here

print(layer1.weight.data)
print(layer1.bias.data)

layer_no_bias = nn.Linear(in_features = 3, out_features = 2, bias = False)

output_tensor = layer_no_bias(input_tensor)
output_tensor

print(layer_no_bias.weight)
print(layer_no_bias.bias)

#A batch of inputs
batch_input = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5, 6], [7., 8, 9], [10., 1, 12]])
batch_output = layer1(batch_input)
batch_output

"""##Creating ANN model using class creation method(nn.Module)"""

#To create a custom model, we need to create in the form of classes using nn.Module
#this is going to be the base class for all our models

class LinearRegressionModel(nn.Module):
  def __init__(self):
    super(LinearRegressionModel, self).__init__()
    self.layer1 = nn.Linear(in_features = 1, out_features = 1)

  def forward(self, x):
    return self.layer1(x)

model = LinearRegressionModel()

X = torch.linspace(0,10,10).reshape(-1,1)
y = 2 * X + 1 + torch.randn(X.size())*2

predictions = model(X) #model(x) ran model.forward() implicitly
predictions

#Lets access weights of layer1 through model
model.layer1.weight

model.layer1.bias
#these are just the randomly initialised weights and bias, now we need to optimise them

"""Creating model by class creation method"""

class SimpleModel(nn.Module):
  def __init__(self):
    super(SimpleModel, self).__init__()
    self.layer1 = nn.Linear(10, 5) #10 inputs and 5 outputs
    self.relu = nn.ReLU()
    self.layer2 = nn.Linear(5, 1)

  def forward(self, x): #sequence of layers are arranged according to the sequence in forward function
    x = self.layer1(x)
    x = self.relu(x)
    x = self.layer2(x)
    return x

net = SimpleModel() # this is the ANN we just created with 3 layers (2 linear and 1 relu)
print(net)

torch.manual_seed(0)
input_tensor = torch.randn(1,10)
output = net(input_tensor) #net.forward(input_tensor)
output

#Weight matrices
print(net.layer1.weight)
print(net.layer1.bias)

print(net.layer2.weight)
print(net.layer2.bias)

net.parameters()

for each in net.parameters():
  print(each)

for each in net.parameters():
  print(each.shape)
#order is first layer weights, bias ten second layer w and b

"""##Creating model using nn.sequential"""

model = nn.Sequential(
    nn.Linear(10, 5),
    nn.ReLU(),
    nn.Linear(5, 1)
)

model

output = model(input_tensor) #net.forward(input_tensor)
output

#Accessing weights and biases
print(model[0].weight)
print(model[0].bias)
print(model[2].weight)
print(model[2].bias)

"""## Optimize our model"""

class SimpleModel(nn.Module):
  def __init__(self):
    super(SimpleModel, self).__init__()
    self.layer1 = nn.Linear(10, 5) #10 inputs and 5 outputs
    self.relu = nn.ReLU()
    self.layer2 = nn.Linear(5, 1)

  def forward(self, x): #sequence of layers are arranged according to the sequence in forward function
    x = self.layer1(x)
    x = self.relu(x)
    x = self.layer2(x)
    return x

net = SimpleModel()
print(net)

inputs = torch.randn(100, 10)
targets = torch.rand(100, 1)

criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr = 0.01)

for epoch in range(2000):
  optimizer.zero_grad()
  outputs = net(inputs) # forward pass
  loss = criterion(outputs, targets) #loss
  loss.backward() # backward pass
  optimizer.step() # weights updation

  if epoch % 100 == 0:
    print(f'Epoch {epoch}, Loss: {loss.item()}')

"""- Zero the gradient
- Forward Pass
- Compute the loss
- Backward Pass
- Optimizer.step() updates the weights


For each variable optimizer.zero_grad() makes gradient to be 0

##Demo ANN Model
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

np.random.seed(42)
X = np.random.rand(100, 1)*10
# y = 3x+2
y = 3 * X + 2 + np.random.randn(100, 1)*0.2

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# X_train_tensor = torch.tensor(X_train, dtype = torch.float32)
# y_train_tensor = torch.tensor(y_train, dtype = torch.float32)

# X_test_tensor = torch.tensor(X_test, dtype = torch.float32)
# y_test_tensor = torch.tensor(y_test, dtype = torch.float32)

# class LinearRegressionModel(nn.Module):
#   def __init__(self):
#     super(LinearRegressionModel, self).__init__()
#     self.layer = nn.Linear(1, 1)

#   def forward(self, x):
#     return self.layer(X)

# model = LinearRegressionModel()
# model

# model.layer.weight #randomly initialised weights, we already know it should be 3

# model.layer.bias #randomly initialised bias, we already know it should be 2

# criterion = nn.MSELoss()
# optimizer = optim.SGD(model.parameters(), lr = 0.01)
# num_epochs = 1000
# losses = []

# for epoch in range(num_epochs):
#   optimizer.zero_grad()
#   outputs = model(X_train_tensor)
#   loss = criterion(outputs, y_train_tensor)
#   losses.append(loss)
#   loss.backward()
#   optimizer.step()

#   if (epoch+1) % 100 == 0:
#     print(f'Epoch: [{epoch+1}/{num_epochs}] , Loss : {loss.item()} ')
#     print(model.layer.weight.item(), model.layer.bias.item())

X_train_tensor = torch.tensor(X_train, dtype = torch.float32)
y_train_tensor = torch.tensor(y_train, dtype = torch.float32)
X_test_tensor = torch.tensor(X_test, dtype = torch.float32)
y_test_tensor = torch.tensor(y_test, dtype = torch.float32)

class LinearRegressionModel(nn.Module):
  def __init__(self):
    super(LinearRegressionModel, self).__init__()
    self.layer = nn.Linear(1, 1)
  def forward(self, x):
    return self.layer(x)

model = LinearRegressionModel()

criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr = 0.01)

num_epochs = 1000
losses = []

for epoch in range(num_epochs):
  optimizer.zero_grad()
  outputs = model(X_train_tensor)
  loss = criterion(outputs, y_train_tensor)
  # losses.append(loss)
  loss.backward()
  optimizer.step()

  if (epoch+1) % 100 == 0:
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')
    print(model.layer.weight.item(), model.layer.bias.item())

with torch.no_grad():
  predictions = model(X_test_tensor)
  test_loss = criterion(predictions, y_test_tensor)

print(test_loss)

model.layer.weight.item() # final learned parameter

model.layer.bias.item()

"""## ANN Model on California Housing Dataset"""

from sklearn.datasets import fetch_california_housing

data = fetch_california_housing()
X, y = data.data, data.target

print(X.shape)
print(y.shape)

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)

X_train_tensor = torch.tensor(X_train, dtype = torch.float32)
y_train_tensor = torch.tensor(y_train, dtype = torch.float32)
X_test_tensor = torch.tensor(X_test, dtype = torch.float32)
y_test_tensor = torch.tensor(y_test, dtype = torch.float32)

class Demo(nn.Module):
  def __init__(self):
    super(Demo,self).__init__()
    self.layer1 = nn.Linear(8,4)
    self.relu = nn.ReLU()
    self.layer2 = nn.Linear(4,1)

  def forward(self, x):
    x = self.layer1(x)
    x = self.relu(x)
    x = self.layer2(x)
    return x

model = Demo()
model

criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr = 0.01)

num_epochs = 1000
losses = []

for epoch in range(num_epochs):
  optimizer.zero_grad()
  outputs = model(X_train_tensor)
  loss = criterion(outputs, y_train_tensor)
  losses.append(loss)
  loss.backward()
  optimizer.step()

  if (epoch+1) % 100 == 0:
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')
    # print("Layer 1:", (model.layer1.weight.data(), model.layer1.bias.data()))
    # print("Layer 2:", model.layer2.weight.data(), model.layer2.bias.data())

print(losses)

with torch.no_grad():
  predictions = model(X_test_tensor)
  test_loss = criterion(predictions, y_test_tensor)

print(test_loss)

"""#CNN

##Initialization
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms #loading datasets and transforming them for preprocessing
from torch.utils.data import Dataset, DataLoader

dir(datasets)

#Checking FashionMNIST dataset
train_set = datasets.FashionMNIST(root = '.data', download = True, train = True)

dir(train_set)
help(train_set)

print(train_set.data)
print(train_set.data.shape)

print(train_set.targets)
print(train_set.classes)

"""##Loading dataset"""

#LOading train and test data of CIFAR10 with tranformations like conversion to tensor and normalization
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

train_data = datasets.CIFAR10(root = 'data', download = True, train = True, transform = transform)
test_data = datasets.CIFAR10(root = 'data', download = True, train = False, transform = transform)

print(train_data.classes) #images shows these 10 class
print(train_data.targets) #total target values from 0 to 9
print(train_data.data.shape) # shape of training data, having 50000 datasets each of 32*32 pixels and 3 layers
print(test_data.data.shape) # shape of test data, having 10000 datasets each of 32*32 pixels and 3 layers

type(train_data.data[0]) #checking type of each input data point

dir(transforms)

transform = transforms.ToTensor() #take features from data setand convert them to tensor from np.ndarray

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

train_data.transform = transform

print(type(train_data.data[0]))
train_data.data[0]

"""DataLoader is used to efficiently load data from the dataset, apply the transformations, and provide the data in batches for training or evaluation."""

trainloader = DataLoader(train_data, batch_size=10, shuffle = True) #training data in batch size of 10
testloader = DataLoader(test_data, batch_size=10, shuffle = False) #Shuffle is false for testing data
iterator = iter(trainloader) #iterates over dataset and gives new batch everytime

features, labels = next(iterator)
print(features.shape) # 10 datasets each of 3 layer/channel and 32 * 32 pixels
print(labels)

x = features[0]
print(x) #this is the data of one image i.e. 3 channels and 32*32 pixels
print(x.shape)

"""##Basic of CNN

IF we have input of Width W1, Height H1 and Depth D1 and total K square filters of size F * F then output is W2 * H2 * D2
* D2 is always equal to number of filters i.e. K
* For 2-D
  - W2 = W1 - F + 1
  - H2 = H1 - F + 1
* With Padding P
  - W2 = W1 - F + 2P + 1
  - H2 = H1 - F + 2P + 1
* With Strides S
  - W2 = (W1 - F + 2P)/S + 1
  - H2 = (H1 - F + 2P)/S + 1

* Output Size = (Input Size - Kernal Size) / Strides + 1

##Creating custom class
"""

class customclass(Dataset):
  def __init__(self, file): #here file is our csv file
    self.data = pd.read_csv(file)

  def __len__(self):
    #length of the dataset
    return len(self.data)

  def __getitem__(self, idx): #assuming last column is target and else all columns are features
    #indexing
    feature = self.data.iloc[idx, :-1].values.astype(np.float32)
    label = self.data.iloc[idx, -1]
    return feature, label

"""##Creating custom class for transformed data"""

class customclass(Dataset):
  def __init__(self, file, transform = None): #here file is our csv file
    self.data = pd.read_csv(file)
    self.transform = transform

  def __len__(self):
    #length of the dataset
    return len(self.data)

  def __getitem__(self, idx): #assuming last colum is target and else all columns are features
    #indexing
    feature = self.data.iloc[idx, :-1].values.astype(np.float32)
    label = self.data.iloc[idx, -1]
    if self.transform:
      feature = self.transform(feature) #if some transformation is given then transform features accordingly
    return feature, label

"""## Demo CNN

- Architecture of network (nn.Module) and Forward pass
- Define loss function (nn)
- Backward Pass (optim)
"""

nn.Linear(4, 5)
conv1 = nn.Conv2d(3, 6, 5)
# input channel is 3 i.e. the depth of input tensor D1 = 3
# output channel is 6 i.e. the number of filters K = 6
# kernal size is 5 i.e. dimension F * F = 5 * 5

x = features[0]
x.shape

"""H_out = H + F + 2p / s + 1"""

x = conv1(x)

x.shape
# Output Channel = Depth D2 = same as number of kernals/filters = K = 6
# Output height = output width = H - F + 1 = 32 - 5 + 1 = 28

pool = nn.MaxPool2d(2, 2)
# for max pooling each channel is pooled sepearetely so input channel and output channel are same and no need to mention.
# Kernal Size F = 2 * 2
# Strides S = 2

x = pool(x)

x.shape
# output channel = same as input channel = 6
# Output Height = Output Width = (input size - Kernal Size)/Strides + 1 = (28 - 2)/2 + 1 = 13 + 1 = 14

conv2 = nn.Conv2d(6, 16, 5)
# Input filter = 6 i.e. equal to output K of previous layer
# Number of filters K = 16
# Size of kernal = F * F = 5 * 5

x = conv2(x)
x

x.shape
# Output channel = Number of filters = 16
# Output Size = 14 - 5/1 + 1 = 10

x = pool(x)
x.shape
# Output Size = (10 - 2)/2 + 1 = 4 + 1 = 5

x = x.view(-1, 16 * 5 * 5) #flattens the matrix
x.shape

#Fully connected layer with input as many numbers of datapoints retained from previous layer and output number id defined according to need/requirement
fc1 = nn.Linear(16*5*5, 80)

x = fc1(x)
print(x.shape)
#Fully connected layers reduces the input into desried number of output

#Next fully connected layer to reduce it further
fc2 = nn.Linear(80,20)
x = fc2(x)
print(x.shape)

#As our target variable had 10 classes to in the final layer lets reduce it to 10 outputs
fc3 = nn.Linear(20,10)
x = fc3(x)
print(x.shape)

"""## Demo CNN using class model nn.Module"""

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.conv1 = nn.Conv2d(3,6,5)
    self.pool = nn.MaxPool2d(2,2)
    self.conv2 = nn.Conv2d(6,16,5)
    self.fc1 = nn.Linear(16*5*5, 80)
    self.fc2 = nn.Linear(80, 20)
    self.fc3 = nn.Linear(20, 10)
    # self.dropout = nn.Dropout(p= 0.5)

  def forward(self, x):
    x = self.conv1(x) #first Convolutional layer
    x = F.relu(x) #relu as an activation function
    x = self.pool(x) #pooling layer with kernal of size 2 * 2 and strids = 2
    x = self.conv2(x)
    x = F.relu(x)
    x = self.pool(x)
    x = x.view(-1, 16*5*5) #flattening
    x = self.fc1(x) #first fully connected layer
    x = F.relu(x)
    x = self.fc2(x)
    x = F.relu(x)
    x = self.fc3(x)
    return x

model = Net()
model

print(model(features))

"""Training of CNN model"""

criterion = nn.CrossEntropyLoss() #loss function for categorical data - cross entropy
optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9) #momentum based gradient descent for model training

"""- 1 epoch = pass through the entire dataset
- 1 iteration = pass through 1 batch

- Total number of training data points = 50000
- We have batch_size = 10
- Total number of batches = 50000/10 = 5000

- In one epoch, parameters will be updated for 5000 times
"""

for epoch in range(3):
  accuracy = 0
  for i, data in enumerate(trainloader):
    input, labels = data
    #input = input.to_device

    optimizer.zero_grad()
    output = model(input)
    loss = criterion(output, labels)
    loss.backward()
    optimizer.step()

    correct_points = torch.argmax(output, axis = 1) == torch.as_tensor(labels)
    accuracy += torch.count_nonzero(correct_points)/10
  print(accuracy/5000)

model.conv1.weight.shape

"""## Evaluating the CNN model"""

model.eval()

test_accuracy = 0
for i, data in enumerate(testloader):
  inputs, labels = data
  outputs = model(inputs)

  correct_classes = torch.argmax(outputs, axis = 1) == torch.as_tensor(labels)
  test_accuracy+= torch.count_nonzero(correct_classes)

print(test_accuracy/10000)

"""## How it looks visually"""

import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

# Download and load the training data
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)

# Function to show an image
def imshow(img):
    img = img / 2 + 0.5  # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Get some random training images
dataiter = iter(trainloader)
images, labels = next(dataiter)  # Use next(dataiter) instead of dataiter.next()

# Show images
imshow(torchvision.utils.make_grid(images))
print(' '.join('%5s' % trainset.classes[labels[j]] for j in range(4)))

import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        conv1_output = x
        x = self.pool(x)
        pool1_output = x
        x = F.relu(self.conv2(x))
        conv2_output = x
        x = self.pool(x)
        pool2_output = x
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x, conv1_output, pool1_output, conv2_output, pool2_output

net = Net()

# Get some random training images
dataiter = iter(trainloader)
images, labels = next(dataiter)

# Forward pass
outputs, conv1_out, pool1_out, conv2_out, pool2_out = net(images)

# Visualize the outputs
def visualize_output(tensor, title):
    tensor = tensor[0].detach()  # Take the first image in the batch and detach from graph
    tensor = tensor - tensor.min()  # Normalize to 0-1
    tensor = tensor / tensor.max()
    npimg = tensor.numpy()
    fig, axes = plt.subplots(1, npimg.shape[0], figsize=(12, 3))
    for i, ax in enumerate(axes):
        ax.imshow(npimg[i], cmap='gray')
        ax.axis('off')
    plt.suptitle(title)
    plt.show()

visualize_output(conv1_out, 'Conv1 Output')
visualize_output(pool1_out, 'Pool1 Output')
visualize_output(conv2_out, 'Conv2 Output')
visualize_output(pool2_out, 'Pool2 Output')

"""# RNN"""

import torch
import torch.nn as nn

input_size = 10
hidden_size = 20

model = nn.RNNCell(input_size, hidden_size)
model

model.weight_hh.shape

model.weight_ih.shape

model.bias_hh.shape

model.bias_ih.shape

model.bias

input = torch.randn(1,10)
out = model(input)
out.shape

input_size = 10
output_size = 20
num_layer = 2

model2 = nn.RNN(input_size, hidden_size, num_layer)
model2

model2.parameters()

for each in model2.parameters():
  print(each.shape)

input2 = torch.randn(1,10)
out2,hidden = model2(input2)
out2

hidden